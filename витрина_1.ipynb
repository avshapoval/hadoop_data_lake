{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7bd51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/14 16:19:45 WARN Utils: Your hostname, fhmmgutu3rdhsp9gl3m3 resolves to a loopback address: 127.0.1.1; using 172.16.0.30 instead (on interface eth0)\n",
      "22/10/14 16:19:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/14 16:19:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import tzwhere\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"10g\")\n",
    "    .config(\"spark.driver.cores\", 35)\n",
    "    .appName(\"de-project-7\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d318cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44abdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#60\n",
    "def input_paths(date: str, depth: int, namenode_url:str, input_path: str):\n",
    "    return [\n",
    "        f'{namenode_url}{input_path}/' \\\n",
    "            f\"date={datetime.strftime((datetime.strptime(date, '%Y-%m-%d') - timedelta(days=day_amount)), '%Y-%m-%d')}\"\n",
    "        for day_amount in range(depth)\n",
    "    ]\n",
    "\n",
    "date = '2022-03-31'\n",
    "depth = 60\n",
    "namenode_url = 'hdfs://rc1a-dataproc-m-dg5lgqqm7jju58f9.mdb.yandexcloud.net:8020'\n",
    "base_input_path = '/user/master/data/geo/events'\n",
    "\n",
    "paths = input_paths(date, depth, namenode_url, base_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8f582d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "geo = spark.read.parquet(f'{namenode_url}/user/enpassan/dev/geo_data_correct')\n",
    "messages = spark.read.parquet(*paths).withColumn(\"event_id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e546ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save corrected csv as parquet\n",
    "# \n",
    "# geo = spark.read.options(header = \"true\", delimiter = \";\").csv(f'{namenode_url}/user/enpassan/dev/geo_data')\\\n",
    "#     .withColumn(\"lat_c\", F.regexp_replace(\"lat\", \",\", \".\").cast(\"double\"))\\\n",
    "#     .withColumn(\"lon_c\", F.regexp_replace(\"lng\", \",\", \".\").cast(\"double\"))\\\n",
    "#     .withColumn(\"id\", F.col(\"id\").cast('int'))\\\n",
    "#     .drop(\"lat\")\\\n",
    "#     .drop(\"lng\")#to_parquet?\n",
    "\n",
    "# geo.write.mode(\"overwrite\").parquet(f'{namenode_url}/user/enpassan/dev/geo_data_correct')\n",
    "geo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717a4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f71dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window().partitionBy(messages.event_id).orderBy(F.asc(F.abs(F.asin(F.sqrt(F.sin((messages.lat - geo.lat_c) / 2) ** 2 \\\n",
    "+ F.cos(messages.lat) * F.cos(geo.lat_c) * \\\n",
    "(F.sin((messages.lon - geo.lon_c) / 2) ** 2))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "mes_geo = messages.join(geo, how='cross').withColumn(\"rn\", F.row_number().over(window)).where(\"rn == 1\").with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "mes_geo.write.mode(\"overwrite\").parquet(f\"{namenode_url}/user/enpassan/dev/mes_geo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "mes_geo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b099cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "mes_geo = spark.read.parquet(f\"{namenode_url}/user/enpassan/dev/mes_geo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d705728",
   "metadata": {},
   "outputs": [],
   "source": [
    "mes_geo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c83d50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_geo = mes_geo\\\n",
    "    .where(\"event_type == 'message'\")\\\n",
    "    .selectExpr(\"event.message_from as user_id\", \"event.datetime as dt\", \"id as city_id\", \"city as city_name\", \"lat_c\", \"lon_c\")\\\n",
    "    .withColumn(\"dt\", F.to_timestamp(\"dt\",\"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b664b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_geo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9842eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_act_city = Window().partitionBy(\"user_id\").orderBy(F.desc(\"dt\"))\n",
    "\n",
    "user_city = user_geo\\\n",
    "    .where(\"dt is not null\")\\\n",
    "    .withColumn(\"rn\", F.row_number().over(window_act_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6216f812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be624a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timezonefinder import TimezoneFinder\n",
    "from pyspark.sql import types as T\n",
    "from cachetools import cached\n",
    "\n",
    "@cached(cache={})\n",
    "def get_tf():\n",
    "    return TimezoneFinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd7772ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType = T.StringType())\n",
    "def get_tz(lng, lat):\n",
    "    tf = get_tf()\n",
    "    tz_str = tf.timezone_at(lng=lng, lat=lat)\n",
    "    return tz_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b43bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tz(143, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d7e43e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_act_city = user_city\\\n",
    "    .where(\"rn == 1\")\\\n",
    "    .withColumn(\"local_time\", F.from_utc_timestamp(\"dt\", get_tz(user_city.lon_c, user_city.lat_c)))\\\n",
    "    .select(\"user_id\", F.expr(\"city_name as act_city\"), \"local_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70f4d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_home_city = user_city\\\n",
    "    .withColumn(\"days_stayed\", (F.col(\"dt\") - F.lag(\"dt\", 1).over(window_act_city)).cast('long')/3600/24 * (-1))\\\n",
    "    .where(\"days_stayed > 27\")\\\n",
    "    .groupBy(\"user_id\", \"city_id\", F.expr(\"city_name as home_city\"))\\\n",
    "    .agg(F.max(\"rn\").alias(\"rn\"))\\\n",
    "    .drop(\"rn\", \"city_id\")\\\n",
    "\n",
    "# home_city - если одна запись, то lag вернет null, иначе количество в днях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f48a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Как считать города без указанного времени отправки сообщения? - неясен порядок посещения городов, такая информация бесполезна\n",
    "\n",
    "user_cities_visited = user_geo\\\n",
    "    .where(\"dt is not null\")\\\n",
    "    .groupBy(\"user_id\")\\\n",
    "    .agg(F.sort_array(F.collect_list(F.struct(\"dt\", \"city_name\")), asc=False).alias(\"collected\"))\\\n",
    "    .withColumn(\"travel_array\", F.col(\"collected.city_name\"))\\\n",
    "    .drop(\"collected\")\\\n",
    "    .withColumn(\"travel_count\", F.size(\"travel_array\"))\n",
    "\n",
    "\n",
    "#     .distinct()\\\n",
    "\n",
    "#     .withColumn(\"travel_count\", F.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f20ebeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_city_info = user_act_city\\\n",
    "    .join(user_home_city, on = \"user_id\", how=\"left\")\\\n",
    "    .join(user_cities_visited, on=\"user_id\", how=\"left\")\n",
    "#Витрина 1 готова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc899280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------------+---------+--------------------+------------+\n",
      "|user_id|   act_city|         local_time|home_city|        travel_array|travel_count|\n",
      "+-------+-----------+-------------------+---------+--------------------+------------+\n",
      "|     65|  Newcastle|2022-02-01 02:13:26|     null|         [Newcastle]|           1|\n",
      "|      7|  Melbourne|2022-03-29 13:22:13|     null|         [Melbourne]|           1|\n",
      "|     84|Rockhampton|2022-02-09 05:08:14|     null|       [Rockhampton]|           1|\n",
      "|     68|  Newcastle|2022-03-26 04:08:56|     null|         [Newcastle]|           1|\n",
      "|     27| Cranbourne|2022-03-27 02:55:40|     null|        [Cranbourne]|           1|\n",
      "|     17|   Canberra|2022-03-26 00:29:27|     null|          [Canberra]|           1|\n",
      "|     41|    Bendigo|2022-02-06 04:44:00|     null|           [Bendigo]|           1|\n",
      "|     85|     Darwin|2022-03-24 15:09:49|     null|            [Darwin]|           1|\n",
      "|     61| Townsville|2022-03-10 15:44:58|     null|        [Townsville]|           1|\n",
      "|      3|Rockhampton|2022-02-26 15:48:03|     null|       [Rockhampton]|           1|\n",
      "|     11|    Bunbury|2022-02-27 23:38:48|     null|           [Bunbury]|           1|\n",
      "|     92|   Adelaide|2022-03-08 17:19:38|     null|          [Adelaide]|           1|\n",
      "|     13| Wollongong|2022-03-06 22:13:53|     null|        [Wollongong]|           1|\n",
      "|     36|   Canberra|2022-02-06 20:47:21|     null|          [Canberra]|           1|\n",
      "|     75|Rockhampton|2022-02-02 20:03:40|     null|       [Rockhampton]|           1|\n",
      "|     18|  Melbourne|2022-02-13 08:29:19|     null|         [Melbourne]|           1|\n",
      "|     14|    Geelong|2022-02-12 07:48:40|     null|           [Geelong]|           1|\n",
      "|     20|    Geelong|2022-02-06 07:21:32|     null|           [Geelong]|           1|\n",
      "|     47|   Canberra|2022-03-16 21:23:19|     null|[Canberra, Wollon...|           2|\n",
      "|     53| Cranbourne|2022-02-21 11:36:09|     null|        [Cranbourne]|           1|\n",
      "+-------+-----------+-------------------+---------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_city_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c46bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mes_geo.printSchema()\n",
    "\n",
    "# mes_geo.write.mode(\"overwrite\").parquet(f\"{namenode_url}/user/enpassan/dev/mes_geo\")\n",
    "\n",
    "# !hdfs dfs -ls /user/enpassan/dev/mes_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667260e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hdfs dfs -du -s -h /user/enpassan/dev/mes_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b906daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mes_geo = spark.read.parquet(f\"{namenode_url}/user/enpassan/dev/mes_geo\").withColumn(\"city_id\", F.col(\"id\")).drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a407832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mes_geo.groupBy(\"city_id\").count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
